{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do\n",
    "\n",
    "Models.py => Load a model (download it if needed), from the list, or custom gensim model\n",
    "\n",
    "Analogy.py => Analogy test results, parameters = model, questions (bats normaly, do we allow mikolov's questions? any kind?), vanilla or not\n",
    "-> Good code already ready, just need it to be cleaner to output well the metrics (.txt or .json?)\n",
    "\n",
    "analogy_decompo.py => Output the decomposition scores, same for diff_sim. Parameters = model, questions\n",
    "\n",
    "metrics.py => OCS and PCS, parameters = model, questions, number of permutations.\n",
    "           => test option; create the random sets, new parameter = nb random sets, limit random words\n",
    "-> Need to adapt the code here, but should be relatively easy\n",
    "\n",
    "-> Option to output results (analogy/metrics) for single model or all in our list.\n",
    "\n",
    "plot.py => Plot results from analogy or metrics or analogy_decompo like in the paper.\n",
    "\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "# Using chrisjmccormick's github for the basic word2vec import\n",
    "\n",
    "import gensim\n",
    "from gensim import utils, matutils\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec, Word2VecKeyedVectors\n",
    "\n",
    "import logging\n",
    "import wget\n",
    "from itertools import chain\n",
    "import logging\n",
    "from six import string_types\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import os\n",
    "import sklearn\n",
    "\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.sparse.linalg import norm\n",
    "from scipy.stats import iqr\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "from svd2vec import svd2vec\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "\n",
    "import tensorflow\n",
    "import transformers\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "#wget.download('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n",
    "\n",
    "# Glove\n",
    "#!wget http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip\n",
    "#!unzip glove.840B.300d.zip\n",
    "\n",
    "# dict2vec\n",
    "#!wget https://dict2vec.s3.amazonaws.com/dict2vec300.tar.bz2\n",
    "#!tar -x dict2vec300.tar.bz2\n",
    "\n",
    "# ConceptNet Numberbatch\n",
    "#!wget https://conceptnet.s3.amazonaws.com/downloads/2019/numberbatch/numberbatch-en-19.08.txt.gz\n",
    "\n",
    "# BERT and GPT-2 ==> Loading will download them\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-large-uncased').embeddings.word_embeddings.weight.data.numpy()\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').transformer.wte.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download question-words\n",
    "#wget.download('https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "pretrained_embeddings_path = \"word-embeddings-geometry/GoogleNews-vectors-negative300.bin.gz\"\n",
    "# \"./GoogleNews-vectors-negative300.bin.gz\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path, \n",
    "binary=True)\n",
    "\n",
    "# Glove\n",
    "tmp_file = get_tmpfile(\"./glove_gensim.txt\")\n",
    "_ = glove2word2vec('./glove.840B.300d.txt', tmp_file)\n",
    "\n",
    "model_glove = gensim.models.KeyedVectors.load_word2vec_format(tmp_file)\n",
    "\n",
    "# dict2vec\n",
    "model_dict2vec = gensim.models.KeyedVectors.load_word2vec_format(\"dict2vec-vectors-dim300.vec\", binary=False, unicode_errors=\"ignore\")\n",
    "\n",
    "# ConceptNet Numberbatch\n",
    "pretrained_embeddings_path = \"numberbatch-en-19.08.txt.gz\"\n",
    "model_conceptnet = gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path)\n",
    "\n",
    "# BERT and GPT-2\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-large-uncased').embeddings.word_embeddings.weight.data.numpy()\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').transformer.wte.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_embedding(tokenizer, model, word):\n",
    "    tokenized_text = tokenizer.tokenize(word)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    embeds = np.array([model[i] for i in indexed_tokens])\n",
    "    embed = np.mean(embeds, axis=0)\n",
    "    return(embed)\n",
    "\n",
    "def permutation_onecycle(n):\n",
    "    if type(n) == tuple:\n",
    "        n1, n2 = n[0], n[1]\n",
    "    else:\n",
    "        n1, n2 = 0, n\n",
    "    l=np.random.permutation(range(n1, n2))\n",
    "    for i in range(n1, n2):\n",
    "        if i==l[i-n1]: \n",
    "            j=np.random.randint(n1, n2)\n",
    "            while j==l[j-n1]:\n",
    "                j=np.random.randint(n1, n2)\n",
    "            l[i-n1], l[j-n1] = l[j-n1], l[i-n1]\n",
    "    return(l)\n",
    "\n",
    "def permutation_onecycle_avoidtrue(n, real): #May be a more optimal way\n",
    "    test=False\n",
    "    perm = permutation_onecycle(n)\n",
    "    for i_r in range(len(real)):\n",
    "        if real[i_r][1] == real[perm[i_r]][1]:\n",
    "            test=True\n",
    "    while test:\n",
    "        test=False\n",
    "        perm = permutation_onecycle(n)\n",
    "        for i_r in range(len(real)):\n",
    "            if real[i_r][1] == real[perm[i_r]][1]:\n",
    "                test=True\n",
    "    return(perm)\n",
    "\n",
    "def shuffled_directions(model, idx_start, idx_end):\n",
    "    perm_list = permutation_onecycle(len(idx_start))\n",
    "    dirs = np.array([[model.wv.get_vector(idx_end[perm_list[i]]) - model.wv.get_vector(idx_start[i])\n",
    "                                          for i in range(len(idx_start))]])\n",
    "    return(dirs)\n",
    "\n",
    "def similarite_offsets(directions_tuples, list_offsets):\n",
    "    sim_offsets = []\n",
    "    for i in range(len(directions_tuples)):\n",
    "        sim_offsets.append([])\n",
    "        list_tuples = list(list_offsets[i])\n",
    "        for j in range(len(list_tuples)):\n",
    "            for k in range(j+1,len(list_tuples)):\n",
    "                sim_offsets[-1].append(cos_sim([list_tuples[j]], [list_tuples[k]])[0][0]) \n",
    "    return(np.array(sim_offsets))\n",
    "\n",
    "def similarite_shuffled_offsets(directions_tuples, list_offsets, alpha=0.1):\n",
    "    sim_offsets = []\n",
    "    idxs = []\n",
    "    for i in range(len(directions_tuples)):\n",
    "        sim_offsets.append([])\n",
    "        list_tuples = list(list_offsets[i])\n",
    "        idx_shuffled_offsets = np.random.choice(len(list_tuples), size=int(alpha*(len(list_tuples))), replace=False) \n",
    "        idx_shuffled_offsets.sort()\n",
    "        idxs.append(idx_shuffled_offsets)\n",
    "        for j in range(len(idx_shuffled_offsets)):\n",
    "            for k in range(j+1, len(idx_shuffled_offsets)):\n",
    "                sim_offsets[-1].append(cos_sim([list_tuples[idx_shuffled_offsets[j]]], [list_tuples[idx_shuffled_offsets[k]]])[0][0]) \n",
    "    return(idxs, np.array(sim_offsets))\n",
    "\n",
    "def mean_direction(offsets):\n",
    "    mean = []\n",
    "    mean_ofunnormalized = []\n",
    "    for d in offsets:\n",
    "        da = np.array(d)\n",
    "        norma_d = (1/np.linalg.norm(da, axis=1))[:,None] * da\n",
    "        mean.append(np.mean(norma_d, axis=0))\n",
    "        mean_ofunnormalized.append(np.mean(da, axis=0))\n",
    "\n",
    "def similarity_to_mean(len_categ, offsets, mean_direction):\n",
    "    similarity_tomean = []\n",
    "    for i in range(len_categ):\n",
    "        similarity_tomean.append([])\n",
    "        list_offsets = list(offsets[i])\n",
    "        for j in range(len(list_offsets)):\n",
    "            similarity_tomean[-1].append(cos_sim([list_offsets[j]], [mean_direction[i]])[0][0])\n",
    "    return(np.array(similarity_tomean))\n",
    "\n",
    "def OCS_PCS(len_categs, nb_perm, similarities, similarities_shuffle):\n",
    "    ocs, pcs = [], []\n",
    "    for i in range(len_categs):\n",
    "        pcs_list = []\n",
    "        for perm in range(nb_perm):\n",
    "            y_true = [1 for j in range(len(similarities[i]))]+[0 for j in range(len(similarities_shuffle[perm][i]))]\n",
    "            y_scores = list(similarities[i])+list(similarities_shuffle[perm][i])\n",
    "            #print(y_true)\n",
    "            #print(y_scores)\n",
    "            auc_temp = sklearn.metrics.roc_auc_score(y_true,y_scores)\n",
    "            pcs_list.append(auc_temp)\n",
    "        pcs.append(np.mean(pcs_list))\n",
    "        ocs.append(np.mean(similarities[i]))\n",
    "    return(ocs, pcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogy pairs loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'directions_tuples' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f771977f7b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                         \u001b[0mdirections_tuples_bats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m used_voc = np.hstack([np.hstack([[i[0] for i in directions_tuples[k]] for k in range(len(directions_tuples))]),\n\u001b[0m\u001b[1;32m     20\u001b[0m                       np.hstack([[i[1] for i in directions_tuples[k]] for k in range(len(directions_tuples))])])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'directions_tuples' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#BATS\n",
    "\n",
    "directions_names = []\n",
    "directions_tuples_bats = []\n",
    "\n",
    "for d in os.listdir('BATS_3.0'):\n",
    "    if d != 'metadata.json':\n",
    "        for f in os.listdir('BATS_3.0/'+str(d)):\n",
    "            directions_names.append(str(f)[:-4])\n",
    "            directions_tuples_bats.append(set())\n",
    "            with utils.open_file('BATS_3.0/'+str(d)+'/'+str(f)) as fin:\n",
    "                for line_no, line in enumerate(fin):\n",
    "                    line = utils.to_unicode(line)\n",
    "                    a, b = [word.lower() for word in line.split()]\n",
    "                    list_b = b.split('/')\n",
    "                    if list_b[0] != a:\n",
    "                        directions_tuples_bats[-1].add((a,list_b[0]))\n",
    "            \n",
    "used_voc = np.hstack([np.hstack([[i[0] for i in directions_tuples_bats[k]] for k in range(len(directions_tuples_bats))]),\n",
    "                      np.hstack([[i[1] for i in directions_tuples_bats[k]] for k in range(len(directions_tuples_bats))])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_keys = model.wv.vocab.keys()\n",
    "vocabulary = set(vocabulary_keys)\n",
    "vocabulary_list = np.array(list(vocabulary_keys))\n",
    "\n",
    "directions_tuples = [[d for d in list(directions_tuples_bats[i]) if d[0] in vocabulary and d[1] in vocabulary] for i in range(len(directions_tuples_bats))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L03 [hyponyms - misc] 50 50\n",
      "L02 [hypernyms - misc] 50 50\n",
      "L01 [hypernyms - animals] 50 45\n",
      "L09 [antonyms - gradable] 50 50\n",
      "L06 [meronyms - part] 50 46\n",
      "L07 [synonyms - intensity] 50 50\n",
      "L08 [synonyms - exact] 50 49\n",
      "L10 [antonyms - binary] 50 50\n",
      "L04 [meronyms - substance] 50 49\n",
      "L05 [meronyms - member] 50 49\n",
      "E02 [country - language] 50 36\n",
      "E05 [name - occupation] 50 27\n",
      "E08 [animal - shelter] 50 50\n",
      "E09 [things - color] 50 50\n",
      "E03 [UK_city - county] 50 25\n",
      "E04 [name - nationality] 50 24\n",
      "E10 [male - female] 50 49\n",
      "E07 [animal - sound] 50 50\n",
      "E06 [animal - young] 50 50\n",
      "E01 [country - capital] 50 37\n",
      "D04 [over+adj_reg] 50 50\n",
      "D09 [verb+tion_irreg] 50 49\n",
      "D10 [verb+ment_irreg] 50 48\n",
      "D02 [un+adj_reg] 50 50\n",
      "D03 [adj+ly_reg] 50 50\n",
      "D07 [verb+able_reg] 50 49\n",
      "D05 [adj+ness_reg] 50 46\n",
      "D06 [re+verb_reg] 50 49\n",
      "D08 [verb+er_irreg] 50 49\n",
      "D01 [noun+less_reg] 50 49\n",
      "I09 [verb_Ving - Ved] 50 50\n",
      "I08 [verb_Ving - 3pSg] 50 50\n",
      "I02 [noun - plural_irreg] 48 46\n",
      "I06 [verb_inf - Ving] 50 50\n",
      "I10 [verb_3pSg - Ved] 50 50\n",
      "I07 [verb_inf - Ved] 50 50\n",
      "I05 [verb_inf - 3pSg] 50 50\n",
      "I01 [noun - plural_reg] 50 50\n",
      "I04 [adj - superlative] 50 50\n",
      "I03 [adj - comparative] 50 50\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(directions_tuples)):\n",
    "    print(directions_names[i], \": \",len([d for d in directions_tuples[i] if d[0] in vocabulary and d[1] in vocabulary]), \" pairs of out \", len(directions_tuples[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing random offsets sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_random = 10\n",
    "size_random_categ = 50\n",
    "limit_word = 10000\n",
    "\n",
    "\n",
    "\n",
    "# a* - a, pour chaque categorie\n",
    "direction_w2vs = np.array([[model.wv.get_vector(i[1]) - model.wv.get_vector(i[0]) \n",
    "                            for i in directions_tuples[k] if i[1] in vocabulary and i[0] in vocabulary] \n",
    "                           for k in range(len(directions_tuples))])\n",
    "\n",
    "\n",
    "# a* - a, a et a* de la même catégorie mais permuté\n",
    "perm_lists_nnp = []\n",
    "direction_w2vs_normal_normal_permutation = []\n",
    "for k_r in range(k_random):\n",
    "    perm_lists_nnp.append([])\n",
    "    direction_w2vs_normal_normal_permutation.append([])\n",
    "    for i in range(len(directions_tuples)):\n",
    "        perm_list = permutation_onecycle(len(directions_tuples[i]))\n",
    "        direction_w2vs_normal_normal_permutation[-1].append([])\n",
    "        ds = list(directions_tuples[i])\n",
    "        for k in range(len(ds)):\n",
    "            di = ds[k]\n",
    "            dj = ds[perm_list[k]]\n",
    "            if di[0] in vocabulary and dj[1] in vocabulary and dj[1] != di[0]:\n",
    "                direction_w2vs_normal_normal_permutation[-1][-1].append(model.wv.get_vector(dj[1]) - model.wv.get_vector(di[0]))\n",
    "                \n",
    "        perm_lists_nnp[-1].append(perm_list)\n",
    "        \n",
    "direction_w2vs_categ_categ_intra = []\n",
    "perm_lists_intra = []\n",
    "for k_r in range(k_random):\n",
    "    perm_list_intra = np.hstack([permutation_onecycle(10),\n",
    "                             permutation_onecycle((10,20)),\n",
    "                             permutation_onecycle((20,30)),\n",
    "                             permutation_onecycle((30,40)),\n",
    "                            ])\n",
    "    perm_lists_intra.append(perm_list_intra)\n",
    "    direction_w2vs_categ_categ_intra.append([])\n",
    "    \n",
    "    for i in range(len(directions_tuples)):\n",
    "        direction_w2vs_categ_categ_intra[-1].append([])\n",
    "        j = perm_list_intra[i]\n",
    "        len_max = min(len(directions_tuples[i]), len(directions_tuples[j]))\n",
    "        for k in range(len_max):\n",
    "            di = list(directions_tuples[i])[k]\n",
    "            dj = list(directions_tuples[j])[k]\n",
    "            if dj[1] != di[0]:\n",
    "                direction_w2vs_categ_categ_intra[-1][-1].append(model.wv.get_vector(dj[1]) - model.wv.get_vector(di[0]))\n",
    "\n",
    "                \n",
    "# a* - a, a et a* de categories différentes, probablement très très grand pour bats!!\n",
    "direction_w2vs_categ_categ = []\n",
    "perm_lists_inter = []\n",
    "for k_r in range(k_random):\n",
    "    perm_list_categ_categ = permutation_onecycle(len(directions_tuples))\n",
    "    perm_lists_inter.append(perm_list_categ_categ)\n",
    "    direction_w2vs_categ_categ.append([])\n",
    "    \n",
    "    for i in range(len(directions_tuples)):\n",
    "        direction_w2vs_categ_categ[-1].append([])\n",
    "        j = perm_list_categ_categ[i]\n",
    "        len_max = min(len(directions_tuples[i]), len(directions_tuples[j]))\n",
    "        for k in range(len_max):\n",
    "            di = list(directions_tuples[i])[k]\n",
    "            dj = list(directions_tuples[j])[k]\n",
    "            if dj[1] != di[0]:\n",
    "                    direction_w2vs_categ_categ[-1][-1].append(model.wv.get_vector(dj[1]) - model.wv.get_vector(di[0]))\n",
    "\n",
    "                    \n",
    "# For half random categories\n",
    "idx_random_categ = [[np.random.choice(limit_word, size=len(directions_tuples[k_d]), replace=False) \n",
    "                     for k in range(k_random)] \n",
    "                    for k_d in range(len(directions_tuples))]\n",
    "idx_random_categ = []\n",
    "for k_d in range(len(directions_tuples)):\n",
    "    idx_random_categ.append([])\n",
    "    for k in range(k_random):\n",
    "        rand_ints = np.random.choice(limit_word, size=len(directions_tuples[k_d]), replace=False)\n",
    "        rand_vos = [vocabulary_list[r] for r in rand_ints if not vocabulary_list[r] in used_voc] #i?\n",
    "        while len(rand_vos) < len(directions_tuples[k_d]):\n",
    "            rand_int = int(np.random.choice(limit_word, size=1, replace=False))\n",
    "            if not vocabulary_list[rand_int] in used_voc and not vocabulary_list[rand_int] in rand_vos:\n",
    "                rand_vos.append(vocabulary_list[rand_int])\n",
    "        idx_random_categ[-1].append(rand_vos)\n",
    "idx_random_categ = np.array(idx_random_categ)\n",
    "\n",
    "# a* - a, a et a* de categories différentes\n",
    "# a* - a, a* d'un ensemble random\n",
    "direction_w2vs_random_normal = np.array([[[model.wv.get_vector(idx_random_categ[k_d][k_r][i]) -\\\n",
    "                                           model.wv.get_vector(list_directions_tuples[k_d][i][0])\n",
    "                                          for i in range(len(list_directions_tuples[k_d])) \n",
    "                                           if list_directions_tuples[k_d][i][0] in vocabulary \n",
    "                                          ] \n",
    "                           for k_d in range(len(list_directions_tuples))] for k_r in range(k_random)])\n",
    "\n",
    "# a* - a, a d'un ensemble random\n",
    "direction_w2vs_normal_random = np.array([[[model.wv.get_vector(list_directions_tuples[k_d][i][1]) -\\\n",
    "                                           model.wv.get_vector(idx_random_categ[k_d][k_r][i])\n",
    "                                          for i in range(len(list_directions_tuples[k_d])) \n",
    "                                           if list_directions_tuples[k_d][i][1] in vocabulary \n",
    "                                          ] \n",
    "                           for k_d in range(len(list_directions_tuples))] for k_r in range(k_random)])\n",
    "\n",
    "\n",
    "# For random->random categories\n",
    "idx_random = [np.random.choice(limit_word, size=size_random_categ, replace=False) for k in range(k_random)]\n",
    "idx_random = np.array([[vocabulary_list[i] for i in idx_random[k] if not vocabulary_list[i] in used_voc] for k in range(k_random)])\n",
    "### 2nd definition may be not needed\n",
    "\n",
    "### May be a better way to change it\n",
    "idx_random2 = []\n",
    "for k in range(k_random):\n",
    "    rand_ints = np.random.choice(limit_word, size=size_random_categ, replace=False) \n",
    "    rand_vos = [vocabulary_list[r] for r in rand_ints if not vocabulary_list[r] in used_voc and not vocabulary_list[r] in idx_random[k]]\n",
    "    while len(rand_vos) < 50:\n",
    "        rand_int = int(np.random.choice(limit_word, size=1, replace=False))\n",
    "        if not vocabulary_list[rand_int] in used_voc and not vocabulary_list[rand_int] in rand_vos and not vocabulary_list[rand_int] in idx_random[k]:\n",
    "            rand_vos.append(vocabulary_list[rand_int])\n",
    "    idx_random2.append(rand_vos)\n",
    "idx_random2 = np.array(idx_random2)\n",
    "\n",
    "# a* - a, a et a* d'ensembles random\n",
    "direction_w2vs_random_random = [np.array([[model.wv.get_vector(idx_random2[k_r][i]) - model.wv.get_vector(idx_random[k_r][i]) \n",
    "                                          for i in range(len(idx_random[k_r]))] \n",
    "                           ]) for k_r in range(k_random)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffled sets for normal and random sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:52: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:65: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_perms = 50\n",
    "\n",
    "# Pour la courbe ROC, on a ici changé la version précédente avec des i!=j mtnt\n",
    "direction_w2vs_shuffle = []\n",
    "for k in range(len(directions_tuples)):\n",
    "    direction_w2vs_shuffle.append([])\n",
    "    for perm in range(nb_perms):\n",
    "        perm_list = permutation_onecycle(len(directions_tuples[k]))\n",
    "        dirs = [model.wv.get_vector(directions_tuples[k][perm_list[i]][1]) - model.wv.get_vector(directions_tuples[k][i][0])\n",
    "                                              for i in range(len(directions_tuples[k]))]\n",
    "        direction_w2vs_shuffle[-1].append(dirs)\n",
    "# Fonctionne aussi pour le normal normal permutation\n",
    "\n",
    "# a* - a, a et a* de categories différentes, même grande catégorie pour bats probablement très très grand pour bats!!\n",
    "direction_w2vs_categ_categ_intra_shuffle  = []\n",
    "for k_r in range(k_random):\n",
    "    print(k_r)\n",
    "    direction_w2vs_categ_categ_intra_shuffle.append([])\n",
    "    perm_list_intra = perm_lists_intra[k_r]\n",
    "    for k in range(len(directions_tuples)):\n",
    "        direction_w2vs_categ_categ_intra_shuffle[-1].append([])\n",
    "        kj = perm_list_intra[k]\n",
    "        len_max = min(len(directions_tuples[k]), len(directions_tuples[kj]))\n",
    "        for perm in range(nb_perms):\n",
    "            perm_list = permutation_onecycle(len_max)\n",
    "            #perm_list = permutation_onecycle_avoidtrue(len_max, directions_tuples[kj])\n",
    "            dirs = [model.wv.get_vector(directions_tuples[kj][perm_list[i]][1]) - model.wv.get_vector(directions_tuples[k][i][0])\n",
    "                                                  for i in range(len_max)]\n",
    "            direction_w2vs_categ_categ_intra_shuffle[-1][-1].append(dirs)\n",
    "\n",
    "        \n",
    "direction_w2vs_categ_categ_shuffle = []\n",
    "for k_r in range(k_random):\n",
    "    print(k_r)\n",
    "    direction_w2vs_categ_categ_shuffle.append([])\n",
    "    perm_list_categ_categ = perm_lists_inter[k_r]\n",
    "    for k in range(len(directions_tuples)):\n",
    "        direction_w2vs_categ_categ_shuffle[-1].append([])\n",
    "        kj = perm_list_categ_categ[k]\n",
    "        len_max = min(len(directions_tuples[k]), len(directions_tuples[kj]))\n",
    "        for perm in range(nb_perms):\n",
    "            perm_list = permutation_onecycle(len_max)\n",
    "            #perm_list = permutation_onecycle_avoidtrue(len_max, directions_tuples[kj])\n",
    "            dirs = [model.wv.get_vector(directions_tuples[kj][perm_list[i]][1]) - model.wv.get_vector(directions_tuples[k][i][0])\n",
    "                                                  for i in range(len_max)]\n",
    "            direction_w2vs_categ_categ_shuffle[-1][-1].append(dirs)\n",
    "        \n",
    "        \n",
    "# a* - a, a d'un ensemble random, shuffle\n",
    "direction_w2vs_random_normal_shuffle = []\n",
    "for k_r in range(k_random):\n",
    "    print(k_r)\n",
    "    direction_w2vs_random_normal_shuffle.append([])\n",
    "    for k in range(len(directions_tuples)):\n",
    "        direction_w2vs_random_normal_shuffle[-1].append([])\n",
    "        len_max = min(len(directions_tuples[k]), len(idx_random_categ[k][k_r]))\n",
    "        for perm in range(nb_perms):\n",
    "            perm_list = permutation_onecycle(len_max)\n",
    "            dirs = [model.wv.get_vector(idx_random_categ[k][k_r][perm_list[i]]) - model.wv.get_vector(directions_tuples[k][i][0])\n",
    "                                                  for i in range(len_max)]\n",
    "            direction_w2vs_random_normal_shuffle[-1][-1].append(dirs)\n",
    "\n",
    "direction_w2vs_normal_random_shuffle = []\n",
    "for k_r in range(k_random):\n",
    "    print(k_r)\n",
    "    direction_w2vs_normal_random_shuffle.append([])\n",
    "    for k in range(len(directions_tuples)):\n",
    "        direction_w2vs_normal_random_shuffle[-1].append([])\n",
    "        len_max = min(len(directions_tuples[k]), len(idx_random_categ[k][k_r]))\n",
    "        for perm in range(nb_perms):\n",
    "            perm_list = permutation_onecycle(len_max)\n",
    "            dirs = [model.wv.get_vector(directions_tuples[k][perm_list[i]][1]) - model.wv.get_vector(idx_random_categ[k][k_r][i])\n",
    "                                                  for i in range(len_max)]\n",
    "            direction_w2vs_normal_random_shuffle[-1][-1].append(dirs)\n",
    "\n",
    "direction_w2vs_random_random_shuffle = [[shuffled_directions(model, idx_random[k_r], idx_random2[k_r]) \n",
    "                                         for perm in range(nb_perm)] \n",
    "                                        for k_r in range(k_random)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity of offsets and shuffled offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP\n",
      "intra\n",
      "inter\n",
      "N -> R\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-241-6176396180d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdirections_similarity_w2vs_categ_categ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimilarite_offsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections_tuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection_w2vs_categ_categ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"N -> R\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdirections_similarity_w2vs_random_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimilarite_offsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections_tuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection_w2vs_random_normal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection_w2vs_random_normal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"R -> N\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdirections_similarity_w2vs_normal_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimilarite_offsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections_tuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection_w2vs_normal_random\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection_w2vs_normal_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Similarite offsets normaux\n",
    "directions_similarity_w2vs = similarite_offsets(directions_tuples, direction_w2vs)\n",
    "print(\"NNP\")\n",
    "directions_similarity_w2vs_normal_normal_permutation = [similarite_offsets(directions_tuples, direction_w2vs_normal_normal_permutation[k]) for k in range(k_random)]\n",
    "print(\"intra\")\n",
    "directions_similarity_w2vs_categ_categ_intra = [similarite_offsets(directions_tuples, direction_w2vs_categ_categ_intra[k]) for k in range(k_random)]\n",
    "print(\"inter\")\n",
    "directions_similarity_w2vs_categ_categ = [similarite_offsets(directions_tuples, direction_w2vs_categ_categ[k]) for k in range(k_random)]\n",
    "print(\"N -> R\")\n",
    "directions_similarity_w2vs_random_normal = [similarite_offsets(directions_tuples, direction_w2vs_random_normal[k]) for k in range(k_random)]\n",
    "print(\"R -> N\")\n",
    "directions_similarity_w2vs_normal_random = [similarite_offsets(directions_tuples, direction_w2vs_normal_random[k]) for k in range(k_random)]\n",
    "print(\"R R\")\n",
    "directions_similarity_w2vs_random_random = [similarite_offsets(['random'], direction_w2vs_random_random[k]) for k in range(k_random)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intra\n"
     ]
    }
   ],
   "source": [
    "# Similarite offsets normaux\n",
    "directions_similarity_w2vs_shuffle = [similarite_offsets(directions_tuples, np.array(direction_w2vs_shuffle)[:,perm]) for perm in range(nb_perms)]\n",
    "#directions_similarity_w2vs_normal_normal_permutation_shuffle = similarite_shuffle_offsets(directions_tuples, direction_w2vs_normal_normal_permutation)\n",
    "\n",
    "print('intra')\n",
    "directions_similarity_w2vs_categ_categ_intra_shuffle = [[similarite_offsets(directions_tuples, np.array(direction_w2vs_categ_categ_intra_shuffle[k_r])[:,perm]) for perm in range(nb_perms)] for k_r in range(k_random)]\n",
    "print('inter')\n",
    "directions_similarity_w2vs_categ_categ_shuffle = [[similarite_offsets(directions_tuples, np.array(direction_w2vs_categ_categ_shuffle[k_r])[:,perm]) for perm in range(nb_perms)] for k_r in range(k_random)]\n",
    "print('N>R')\n",
    "directions_similarity_w2vs_random_normal_shuffle = [[similarite_offsets(directions_tuples, np.array(direction_w2vs_random_normal_shuffle[k_r])[:,perm]) for perm in range(nb_perms)] for k_r in range(k_random)]\n",
    "print('R>N')\n",
    "directions_similarity_w2vs_normal_random_shuffle = [[similarite_offsets(directions_tuples, np.array(direction_w2vs_normal_random_shuffle[k_r])[:,perm]) for perm in range(nb_perms)] for k_r in range(k_random)]\n",
    "print('R>R')\n",
    "directions_similarity_w2vs_random_random_shuffle = [[similarite_offsets(['random'], direction_w2vs_random_random_shuffle[k_r][perm]) for perm in range(nb_perms)] for k_r in range(k_random)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCS and PCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_categs = len(directions_names)\n",
    "\n",
    "ocs, pcs = OCS_PCS(len_categs, \n",
    "                   nb_perms,\n",
    "                   directions_similarity_w2vs, \n",
    "                   directions_similarity_w2vs_shuffle)\n",
    "print('nnp')\n",
    "metrics_res = np.array([OCS_PCS(len_categs, \n",
    "                                nb_perms,\n",
    "                                directions_similarity_w2vs_normal_normal_permutation[kr], \n",
    "                                directions_similarity_w2vs_shuffle) for kr in range(k_random)])\n",
    "ocs_nnp, pcs_nnp = metrics_res[:,0], metrics_res[:,1]\n",
    "\n",
    "print('intra')\n",
    "metrics_res = np.array([OCS_PCS(len_categs, \n",
    "                                nb_perms,\n",
    "                                directions_similarity_w2vs_categ_categ_intra[kr], \n",
    "                                directions_similarity_w2vs_categ_categ_intra_shuffle[kr]) for kr in range(k_random)])\n",
    "ocs_categ_categ_intra, pcs_categ_categ_intra = metrics_res[:,0], metrics_res[:,1]\n",
    "\n",
    "print('inter')\n",
    "metrics_res = np.array([OCS_PCS(len_categs, \n",
    "                                nb_perms,\n",
    "                                directions_similarity_w2vs_categ_categ[kr], \n",
    "                                directions_similarity_w2vs_categ_categ_shuffle[kr]) for kr in range(k_random)])\n",
    "ocs_categ_categ, pcs_categ_categ = metrics_res[:,0], metrics_res[:,1]\n",
    "\n",
    "print('N>R')\n",
    "metrics_res = np.array([OCS_PCS(len_categs, \n",
    "                                nb_perms,\n",
    "                                directions_similarity_w2vs_random_normal[kr], \n",
    "                                directions_similarity_w2vs_random_normal_shuffle[kr]) for kr in range(k_random)])\n",
    "ocs_random_normal, pcs_random_normal = metrics_res[:,0], metrics_res[:,1]\n",
    "\n",
    "print('R>N')\n",
    "metrics_res = np.array([OCS_PCS(len_categs, \n",
    "                                nb_perms,\n",
    "                                directions_similarity_w2vs_normal_random[kr], \n",
    "                                directions_similarity_w2vs_normal_random_shuffle[kr]) for kr in range(k_random)])\n",
    "ocs_normal_random, pcs_normal_random = metrics_res[:,0], metrics_res[:,1]\n",
    "\n",
    "print('R>R')\n",
    "metrics_res = np.array([OCS_PCS(1, \n",
    "                                nb_perms,\n",
    "                                directions_similarity_w2vs_random_random[kr], \n",
    "                                directions_similarity_w2vs_random_random_shuffle[kr]) for kr in range(k_random)])\n",
    "ocs_random_random, pcs_random_random = metrics_res[:,0], metrics_res[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version modifiée de gensim, peut être à verifier quels sont les changements exactement (le but est principalement d'autorizer la version vanilla)\n",
    "\n",
    "def most_similar(model, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None, ignore=True):\n",
    "    if positive is None:\n",
    "        positive = []\n",
    "    if negative is None:\n",
    "        negative = []\n",
    "        \n",
    "    model.init_sims()\n",
    "    \n",
    "    if isinstance(positive, string_types) and not negative:\n",
    "        # allow calls like most_similar('dog'), as a shorthand for most_similar(['dog'])\n",
    "        positive = [positive]\n",
    "\n",
    "    # add weights for each word, if not already present; default to 1.0 for positive and -1.0 for negative words\n",
    "    positive = [\n",
    "        (word, 1.0) if isinstance(word, string_types + (np.ndarray,)) else word\n",
    "        for word in positive\n",
    "    ]\n",
    "    negative = [\n",
    "        (word, -1.0) if isinstance(word, string_types + (np.ndarray,)) else word\n",
    "        for word in negative\n",
    "    ]\n",
    "\n",
    "    # compute the weighted average of all words\n",
    "    all_words, mean = set(), []\n",
    "    for word, weight in positive + negative:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(weight * word)\n",
    "        else:\n",
    "            mean.append(weight * model.word_vec(word, use_norm=True))\n",
    "            if word in model.vocab:\n",
    "                all_words.add(model.vocab[word].index)\n",
    "    if not mean:\n",
    "        raise ValueError(\"cannot compute similarity with no input\")\n",
    "    mean = matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "\n",
    "    if indexer is not None:\n",
    "        return indexer.most_similar(mean, topn)\n",
    "\n",
    "    limited = model.vectors_norm if restrict_vocab is None else model.vectors_norm[:restrict_vocab]\n",
    "    dists = np.dot(limited, mean)\n",
    "    if not topn:\n",
    "        return dists\n",
    "    best = matutils.argsort(dists, topn=topn + len(all_words), reverse=True)\n",
    "    # ignore (don't return) words from the input\n",
    "    if ignore:\n",
    "        result = [(model.index2word[sim], float(dists[sim])) for sim in best if sim not in all_words]\n",
    "    else:\n",
    "        result = [(model.index2word[sim], float(dists[sim])) for sim in best]\n",
    "    return result[:topn]\n",
    "\n",
    "def evaluate_word_analogies_bats(model, directory, method='add', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, rank=False):\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    print(\"Method used: \", method)\n",
    "\n",
    "    method1 = 'add'\n",
    "    method2 = 'vanilla'\n",
    "    \n",
    "    ok_vocab = [(w, model.vocab[w]) for w in model.index2word[:restrict_vocab]]\n",
    "    ok_vocab = {w.upper(): v for w, v in reversed(ok_vocab)} if case_insensitive else dict(ok_vocab)\n",
    "    oov = 0\n",
    "    #logger.info(\"Evaluating word analogies for top %i words in the model on %s\", restrict_vocab, analogies)\n",
    "    sections, section = [], None\n",
    "    quadruplets_no = 0\n",
    "    \n",
    "    directions_names_bats = []\n",
    "    directions_tuples_bats = []\n",
    "    \n",
    "    for f in os.listdir('../BATS_3.0/'+str(directory)):\n",
    "        directions_names_bats.append(str(f)[:-4])\n",
    "        directions_tuples_bats.append(set())\n",
    "        with utils.open_file('../BATS_3.0/'+str(directory)+'/'+str(f)) as fin:\n",
    "            for line_no, line in enumerate(fin):\n",
    "                line = utils.to_unicode(line)\n",
    "                a, b = [word.lower() for word in line.split()]\n",
    "                list_b = b.split('/')\n",
    "                if list_b[0] != a:\n",
    "                    directions_tuples_bats[-1].add((a.upper(),list_b[0].upper()))\n",
    "    \n",
    "    for i in range(len(directions_names_bats)):\n",
    "        if section:\n",
    "            # store the last section, too\n",
    "            sections.append(section)\n",
    "            #model._log_evaluate_word_analogies(section)\n",
    "            correct, incorrect = len(section['correct']), len(section['incorrect'])\n",
    "            if correct + incorrect > 0:\n",
    "                score = correct / (correct + incorrect)\n",
    "                logger.info(\"%s: %.1f%% (%i/%i)\", section['section'], 100.0 * score, correct, correct + incorrect)\n",
    "            else:\n",
    "                print('No score for ', section['section'])\n",
    "            correct, incorrect = len(section['correct_vanilla']), len(section['incorrect_vanilla'])\n",
    "            if correct + incorrect > 0:\n",
    "                score = correct / (correct + incorrect)\n",
    "                logger.info(\"%s: %.1f%% (%i/%i) VANILLA\", section['section'], 100.0 * score, correct, correct + incorrect)\n",
    "            if method2 == 'vanilla':\n",
    "                total_section = len(section['correct_vanilla']) + len(section['incorrect_vanilla'])\n",
    "                if total_section > 0:\n",
    "                    logger.info('Number of predictions equal to a: %i (%d), a*: %i (%d), b: %i (%d)', \n",
    "                                section['n_a'], section['n_a']/total_section, \n",
    "                                section['n_a*'], section['n_a*']/total_section, \n",
    "                                section['n_b'], section['n_b']/total_section)\n",
    "\n",
    "        \n",
    "        section = {'section': directions_names_bats[i], 'correct': [], 'incorrect': [], \n",
    "                   'correct_vanilla': [], 'incorrect_vanilla': [],'n_a':0, 'n_a*':0, 'n_b':0,\n",
    "                  'cd': [], 'badc': [], 'bac': [], 'n/cba': [], 'n/c': [], 'n/d': []}\n",
    "        \n",
    "        tuples = directions_tuples_bats[i]\n",
    "        for t1 in tuples:\n",
    "            for t2 in tuples:\n",
    "                a,b = t1\n",
    "                c,expected = t2\n",
    "                if a != c:\n",
    "                    quadruplets_no += 1\n",
    "                    if a not in ok_vocab or b not in ok_vocab or c not in ok_vocab or expected not in ok_vocab:\n",
    "                        oov += 1\n",
    "                        #if dummy4unknown:\n",
    "                        #    logger.debug('Zero accuracy for line #%d with OOV words: %s', line_no, line.strip())\n",
    "                        #    section['incorrect'].append((a, b, c, expected))\n",
    "                        #else:\n",
    "                        #    logger.debug(\"Skipping line with OOV words\")\n",
    "                        continue\n",
    "                    original_vocab = model.vocab\n",
    "                    model.vocab = ok_vocab\n",
    "\n",
    "                    predicted = None\n",
    "                    ignore = {a, b, c} # input words to be ignored\n",
    "                    ignore_bool = False\n",
    "                    positive = [b, c]\n",
    "                    negative=[a]\n",
    "                    # find the most likely prediction using 3CosAdd (vector offset) method\n",
    "                    # TODO: implement 3CosMul and set-based methods for solving analogies\n",
    "                    sims = most_similar(model, positive=positive, negative=negative, topn=5, restrict_vocab=restrict_vocab, ignore=ignore_bool)\n",
    "\n",
    "                    model.vocab = original_vocab\n",
    "\n",
    "                    #predicted = sims[0][0].upper() if case_insensitive else sims[0][0]\n",
    "                    for element in sims:\n",
    "                        predicted = element[0].upper() if case_insensitive else element[0]\n",
    "\n",
    "                        if predicted in ok_vocab  and predicted not in ignore:\n",
    "                            break\n",
    "                    for element in sims:\n",
    "                        predicted_ignore = element[0].upper() if case_insensitive else element[0]\n",
    "\n",
    "                        if predicted_ignore in ok_vocab:\n",
    "                            break\n",
    "                            #if predicted != expected:\n",
    "                                #logger.debug(\"%s: expected %s, predicted %s\", line.strip(), expected, predicted)\n",
    "\n",
    "                    if predicted == expected:\n",
    "                        section['correct'].append((a, b, c, expected))\n",
    "                    else:\n",
    "                        section['incorrect'].append((a, b, c, expected))\n",
    "                        \n",
    "                    if predicted_ignore == expected:\n",
    "                        section['correct_vanilla'].append((a, b, c, expected))\n",
    "                    else:\n",
    "                        section['incorrect_vanilla'].append((a, b, c, expected))\n",
    "                    if predicted_ignore == a:\n",
    "                        section['n_a'] +=1\n",
    "                    if predicted_ignore == b:\n",
    "                        section['n_a*'] +=1\n",
    "                    if predicted_ignore == c:\n",
    "                        section['n_b'] +=1\n",
    "                    \n",
    "    if section:\n",
    "        # store the last section, too\n",
    "        sections.append(section)\n",
    "        #model._log_evaluate_word_analogies(section)\n",
    "        correct, incorrect = len(section['correct']), len(section['incorrect'])\n",
    "        if correct + incorrect > 0:\n",
    "            score = correct / (correct + incorrect)\n",
    "            logger.info(\"%s: %.1f%% (%i/%i)\", section['section'], 100.0 * score, correct, correct + incorrect)\n",
    "        else:\n",
    "            print('No score for ', section['section'])\n",
    "        correct, incorrect = len(section['correct_vanilla']), len(section['incorrect_vanilla'])\n",
    "        if correct + incorrect > 0:\n",
    "            score = correct / (correct + incorrect)\n",
    "            logger.info(\"%s: %.1f%% (%i/%i) VANILLA\", section['section'], 100.0 * score, correct, correct + incorrect)\n",
    "        if method2 == 'vanilla':\n",
    "            total_section = len(section['correct_vanilla']) + len(section['incorrect_vanilla'])\n",
    "            if total_section > 0:\n",
    "                logger.info('Number of predictions equal to a: %i (%d), a*: %i (%d), b: %i (%d)', \n",
    "                            section['n_a'], section['n_a']/total_section, \n",
    "                            section['n_a*'], section['n_a*']/total_section, \n",
    "                            section['n_b'], section['n_b']/total_section)\n",
    "\n",
    "    total = {\n",
    "        'section': 'Total accuracy',\n",
    "        'correct': list(chain.from_iterable(s['correct'] for s in sections)),\n",
    "        'incorrect': list(chain.from_iterable(s['incorrect'] for s in sections)),\n",
    "        'correct_vanilla': list(chain.from_iterable(s['correct_vanilla'] for s in sections)),\n",
    "        'incorrect_vanilla': list(chain.from_iterable(s['incorrect_vanilla'] for s in sections)),\n",
    "    }\n",
    "\n",
    "    oov_ratio = float(oov) / quadruplets_no * 100\n",
    "    logger.info('Quadruplets with out-of-vocabulary words: %.1f%%', oov_ratio)\n",
    "    if not dummy4unknown:\n",
    "        logger.info(\n",
    "            'NB: analogies containing OOV words were skipped from evaluation! '\n",
    "            'To change this behavior, use \"dummy4unknown=True\"'\n",
    "        )\n",
    "    #analogies_score = model._log_evaluate_word_analogies(total)\n",
    "    correct, incorrect = len(total['correct']), len(total['incorrect'])\n",
    "    #print(total)\n",
    "    if correct + incorrect > 0:\n",
    "        score = correct / (correct + incorrect)\n",
    "        logger.info(\"%s: %.1f%% (%i/%i)\", total['section'], 100.0 * score, correct, correct + incorrect)\n",
    "        analogies_score = score\n",
    "    correct_vanilla, incorrect_vanilla = len(total['correct_vanilla']), len(total['incorrect_vanilla'])\n",
    "    #print(total)\n",
    "    if correct_vanilla + incorrect_vanilla > 0:\n",
    "        score = correct_vanilla / (correct_vanilla + incorrect_vanilla)\n",
    "        logger.info(\"%s: %.1f%% (%i/%i) VANILLA\", total['section'], 100.0 * score, correct_vanilla, correct_vanilla + incorrect_vanilla)\n",
    "        analogies_score = score\n",
    "    \n",
    "    sections.append(total)\n",
    "    # Return the overall score and the full lists of correct and incorrect analogies\n",
    "    return analogies_score, sections\n",
    "\n",
    "def bats_test(model):\n",
    "    results = []\n",
    "    for d in os.listdir('../BATS_3.0'):\n",
    "        if d != 'metadata.json':\n",
    "            results.append(evaluate_word_analogies_bats(model.wv, directory=d))\n",
    "    return(results)\n",
    "\n",
    "w2v_results = bats_test(model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Décompo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_w2vs = np.array([[model.wv.get_vector(i[0]) for i in directions_tuples[k] if i[0] in vocabulary and i[1] in vocabulary] \n",
    "                           for k in range(len(directions_tuples))])\n",
    "\n",
    "# a* d'une catégorie\n",
    "d_w2vs = np.array([[model.wv.get_vector(i[1]) for i in directions_tuples[k] if i[0] in vocabulary and i[1] in vocabulary] \n",
    "                           for k in range(len(directions_tuples))])\n",
    "\n",
    "am1_dm1_c_d = []\n",
    "am1_dm1_bma_d = []\n",
    "am1_dm1_bma_c = []\n",
    "am2_ana_bma = []\n",
    "am2_ana_c = []\n",
    "am2_c_bma = []\n",
    "am2_bma_bma = []\n",
    "am1_cd_f_dm1mcm1 = []\n",
    "am1_dm1_bma_dmc = []\n",
    "am1_dm1_c_dmc = []\n",
    "\n",
    "for i in range(len(directions_tuples)):\n",
    "    am1_dm1_c_d.append([])\n",
    "    am1_dm1_bma_d.append([])\n",
    "    am1_dm1_bma_c.append([])\n",
    "    am2_ana_bma.append([])\n",
    "    am2_ana_c.append([])\n",
    "    am2_c_bma.append([])\n",
    "    am2_bma_bma.append([])\n",
    "    am1_cd_f_dm1mcm1.append([])\n",
    "    am1_dm1_bma_dmc.append([])\n",
    "    am1_dm1_c_dmc.append([])\n",
    "    \n",
    "    list_c_w2vs = list(c_w2vs[i])\n",
    "    list_d_w2vs = list(d_w2vs[i])\n",
    "    \n",
    "    for j in range(len(list_c_w2vs)):\n",
    "        for k in range(len(list_c_w2vs)):\n",
    "            if j!=k:\n",
    "                a,b,c,d = list_c_w2vs[j], list_d_w2vs[j], list_c_w2vs[k], list_d_w2vs[k]\n",
    "                norme_a_m1 = 1/(np.linalg.norm(c+b-a))\n",
    "                norme_d_m1 = 1/(np.linalg.norm(d))\n",
    "                norme_c_m1 = 1/(np.linalg.norm(c))\n",
    "                norme_d_m1 = 1/(np.linalg.norm(d))\n",
    "                \n",
    "                am1_dm1_c_d[-1].append(c@d*norme_a_m1*norme_d_m1)\n",
    "                am1_dm1_bma_d[-1].append((b-a)@d*norme_a_m1*norme_d_m1)\n",
    "                am1_dm1_bma_c[-1].append((b-a)@c*norme_a_m1*norme_d_m1)\n",
    "                am2_ana_bma[-1].append((norme_a_m1**2)*(c+b-a)@(b-a))\n",
    "                am2_ana_c[-1].append((norme_a_m1**2)*(c+b-a)@c)\n",
    "                am2_c_bma[-1].append((norme_a_m1**2)*(b-a)@c)\n",
    "                am2_bma_bma[-1].append((norme_a_m1**2)*(b-a)@(b-a))\n",
    "                am1_cd_f_dm1mcm1[-1].append(norme_a_m1*(norme_d_m1 - norme_c_m1)*(c@(c+b-a)))\n",
    "                am1_dm1_bma_dmc[-1].append(norme_a_m1*norme_d_m1*(b-a)@(d-c))\n",
    "                am1_dm1_c_dmc[-1].append(norme_a_m1*norme_d_m1*c@(d-c))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1='#FFBBB3'\n",
    "c2='#5F2EFF'\n",
    "c3='#00610F'\n",
    "idx_idel = [37, 32, 39, 38, 36, 33, 35, 31, 30, 34, 29, 23, 24, 20, 26, 27, 25, 28, 21, 22,  19, 10, 14, 15, 11, 18, 17,\n",
    "       12, 13, 16, 2,  1,  0,  8,\n",
    "        9,  4,  5,  6,  3,  7]\n",
    "x=np.array([d[5:-1] for d in directions_names])[idx_idel]\n",
    "\n",
    "y1=[np.mean(am1_dm1_c_d[i]) for i in idx_idel]\n",
    "y2=[np.mean(am1_dm1_bma_d[i]) for i in idx_idel]\n",
    "y3=[np.mean(am1_dm1_bma_c[i]) for i in idx_idel]\n",
    "y4=np.array(y2) - np.array(y3)\n",
    "#   y4=[np.mean(am1_dm1_c_dmc[i]) for i in idx_idel] même chose sauf erreur\n",
    "fig = go.Figure(go.Bar(x=x, y=y1,name='$b\\cdot b^*$', marker_color= c2))\n",
    "fig.add_trace(go.Bar(x=x, y=y3, name='$b\\cdot o_a$',marker_color= c1))\n",
    "fig.add_trace(go.Bar(x=x, y=y4,name='$o_b\\cdot o_a$', marker_color= c3))\n",
    "fig.update_layout(barmode='relative', \n",
    "                  xaxis={'tickangle':-45, 'ticklen':0.5}, \n",
    "                  font=dict(family=\"Times New Roman\",size=16), \n",
    "                  yaxis_title_text='Value in the analogy score')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "z1= [np.mean(am2_ana_bma[i]) for i in idx_idel]\n",
    "z2= [np.mean(am2_ana_c[i]) for i in idx_idel]\n",
    "z3= [np.mean(am2_c_bma[i]) for i in idx_idel]\n",
    "z4= [np.mean(am2_bma_bma[i]) for i in idx_idel]\n",
    "fig = go.Figure(go.Bar(x=x, y=z2, name='$b\\cdot (b+o_a)$', marker_color= c2))\n",
    "fig.add_trace(go.Bar(x=x, y=z3, name='$b\\cdot o_a$', marker_color= c1))\n",
    "fig.add_trace(go.Bar(x=x, y=z4, name='$off_a\\cdot o_a$', marker_color= c3))\n",
    "fig.update_layout(barmode='relative', \n",
    "                  xaxis={'tickangle':-45, 'ticklen':0.5}, \n",
    "                  font=dict(family=\"Times New Roman\",size=16), \n",
    "                  yaxis_title_text='Value in the reference analogy score')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "y1=[np.mean(am1_cd_f_dm1mcm1[i]) for i in idx_idel]#range(len(x))]\n",
    "y2=[np.mean(am1_dm1_bma_dmc[i]) for i in idx_idel]#range(len(x))]\n",
    "y3=[np.mean(am1_dm1_c_dmc[i]) for i in idx_idel]#range(len(x))]\n",
    "fig = go.Figure(go.Bar(x=x, y=y1, name='$(1\\!-\\!\\|b^*\\|/\\|b\\|)\\!\\cdot\\!(b\\!+\\!o_a)\\!\\cdot\\!b$', marker_color=c1))\n",
    "fig.add_trace(go.Bar(x=x, y=y2, name='$o_a\\!\\cdot\\!o_b$', marker_color=c3))\n",
    "fig.add_trace(go.Bar(x=x, y=y3, name='$b\\!\\cdot\\!o_b$', marker_color=c2))\n",
    "fig.update_layout(barmode='relative', \n",
    "                  xaxis={'tickangle':-45, 'tickwidth':0.5}, \n",
    "                  font=dict(family=\"Times New Roman\",size=16),\n",
    "                  yaxis_title_text='$\\Delta_{sim}$')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivationnal\n",
      "Normal                 0.15630814 0.6790253152647141\n",
      "Permuted within categ  0.0013571794144809246 0.005040067772658974\n",
      "Mismatched within type 0.03354356996715069 0.007551503972635143\n",
      "Mismatched across type 0.10050375675782561 0.006171112820321556\n",
      "Random start           0.004677556827664376 0.0060845519141245865\n",
      "Random end             0.005111376661807299 0.005753678529607292\n",
      "Encyclopedic\n",
      "Normal                 0.19812895 0.5566167896026564\n",
      "Permuted within categ  0.004359106719493866 0.009409635662129967\n",
      "Mismatched within type 0.09686008542776108 0.013313107158677251\n",
      "Mismatched across type 0.07516164667904376 0.006679870954383838\n",
      "Random start           0.009957562014460564 0.008362772750721354\n",
      "Random end             0.007240977883338928 0.008493071611089142\n",
      "Inflectional\n",
      "Normal                 0.2951204 0.8509698953960825\n",
      "Permuted within categ  0.0027671211399137976 0.0074141229504471\n",
      "Mismatched within type 0.08488833475857974 0.006653726813466787\n",
      "Mismatched across type 0.09938082974404097 0.0064051837713574086\n",
      "Random start           0.006108806189149618 0.005884684478369639\n",
      "Random end             0.0068841440137475726 0.004046009532415617\n",
      "Lexicographic\n",
      "Normal                 0.031050116 0.5379290215368527\n",
      "Permuted within categ  0.0008993944153189659 0.0035192740393259537\n",
      "Mismatched within type 0.04553625863045454 0.0049775590259839185\n",
      "Mismatched across type 0.07197023658081889 0.00583456860938319\n",
      "Random start           0.004850496910512448 0.003940537691320761\n",
      "Random end             0.004417304019443691 0.004775911469833643\n",
      "Random start and end   0.002181130555982236 0.0016892413717459753\n"
     ]
    }
   ],
   "source": [
    "#word2vec roc\n",
    "\n",
    "labels=[\"Derivationnal\",\"Encyclopedic\",\"Inflectional\",\"Lexicographic\"]\n",
    "intervals=[(20,30),(10,20),(30,40),(0,10)]\n",
    "x = (ocs, ocs_nnp, ocs_categ_categ_intra, ocs_categ_categ, ocs_random_normal, ocs_normal_random, ocs_random_random)\n",
    "y = (pcs, pcs_nnp, pcs_categ_categ_intra, pcs_categ_categ, pcs_random_normal, pcs_normal_random, pcs_random_random)\n",
    "\n",
    "\n",
    "x_n, x_nnp, x_categ_categ_intra, x_categ_categ, x_random_normal, x_normal_random, x_random_random = x\n",
    "x_nnp, x_categ_categ_intra, x_categ_categ = np.mean(x_nnp,axis=0), np.mean(x_categ_categ_intra,axis=0), np.mean(x_categ_categ,axis=0)\n",
    "x_random_normal, x_normal_random, x_random_random = np.mean(x_random_normal,axis=0), np.mean(x_normal_random,axis=0), np.mean(x_random_random,axis=0)\n",
    "#x_nnp, x_categ_categ_intra, x_categ_categ = iqr(x_nnp,axis=0), iqr(x_categ_categ_intra,axis=0), iqr(x_categ_categ,axis=0)\n",
    "#x_random_normal, x_normal_random, x_random_random = iqr(x_random_normal,axis=0), iqr(x_normal_random,axis=0), iqr(x_random_random,axis=0)\n",
    "\n",
    "y_n, y_nnp, y_categ_categ_intra, y_categ_categ, y_random_normal, y_normal_random, y_random_random = y\n",
    "y_nnp, y_categ_categ_intra, y_categ_categ = np.mean(y_nnp,axis=0), np.mean(y_categ_categ_intra,axis=0), np.mean(y_categ_categ,axis=0)\n",
    "y_random_normal, y_normal_random, y_random_random = np.mean(y_random_normal,axis=0), np.mean(y_normal_random,axis=0), np.mean(y_random_random,axis=0)\n",
    "#y_nnp, y_categ_categ_intra, y_categ_categ = iqr(y_nnp,axis=0), iqr(y_categ_categ_intra,axis=0), iqr(y_categ_categ,axis=0)\n",
    "#y_random_normal, y_normal_random, y_random_random = iqr(y_random_normal,axis=0), iqr(y_normal_random,axis=0), iqr(y_random_random,axis=0)\n",
    "\n",
    "\n",
    "def l_m(x):\n",
    "    return([np.mean(x)])\n",
    "    \n",
    "for i in range(len(labels)):\n",
    "    i1,i2 = intervals[i]\n",
    "    #x_cc = x_categ_categ[:,i1:i2]\n",
    "    #y_cc = x_categ_categ[:,i1:i2]\n",
    "    #for l in perm_lists_inter:\n",
    "        \n",
    "    #for icc in perm_list_categ_categ:\n",
    "    #    if perm_list_categ_categ[icc] in range(i1,i2):\n",
    "    #        x_cc = np.hstack((x_cc, x_categ_categ[icc]))\n",
    "    #        y_cc = np.hstack((y_cc, y_categ_categ[icc]))\n",
    "    print(labels[i])\n",
    "    print(\"Normal                \", l_m(x_n[i1:i2])[0], l_m(y_n[i1:i2])[0])\n",
    "    print(\"Permuted within categ \", l_m(x_nnp[i1:i2])[0], l_m(y_nnp[i1:i2])[0])\n",
    "    print(\"Mismatched within type\", l_m(x_categ_categ_intra[i1:i2])[0], l_m(y_categ_categ_intra[i1:i2])[0])\n",
    "    print(\"Mismatched across type\", l_m(x_categ_categ[i1:i2])[0], l_m(y_categ_categ[i1:i2])[0])\n",
    "    print(\"Random start          \", l_m(x_normal_random[i1:i2])[0], l_m(y_normal_random[i1:i2])[0])\n",
    "    print(\"Random end            \", l_m(x_random_normal[i1:i2])[0], l_m(y_random_normal[i1:i2])[0])\n",
    "print(\"Random start and end  \", l_m(x_random_random)[0], l_m(y_random_random)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicographic\n",
      "Normal\n",
      "[0.031050116]\n",
      "[0.5374205051995696]\n",
      "Normal NNP\n",
      "[0.014700961]\n",
      "[0.5000064274469275]\n",
      "CC intra\n",
      "[0.09704797]\n",
      "[0.500972445498127]\n",
      "CC inter\n",
      "[0.16666335]\n",
      "[0.5020916179998232]\n",
      "Random->Normal\n",
      "[0.06861267398111523]\n",
      "[0.5000440911490568]\n",
      "Normal->Random\n",
      "[0.058894394487142565]\n",
      "[0.4994701837967577]\n",
      "Encyclopedic\n",
      "Normal\n",
      "[0.19812895]\n",
      "[0.5565923947940875]\n",
      "Normal NNP\n",
      "[0.16895457]\n",
      "[0.49930119761268843]\n",
      "CC intra\n",
      "[0.2589875]\n",
      "[0.49648172922058953]\n",
      "CC inter\n",
      "[0.20307705]\n",
      "[0.49914245428597975]\n",
      "Random->Normal\n",
      "[0.20400208070874215]\n",
      "[0.4996925414543981]\n",
      "Normal->Random\n",
      "[0.1374869143217802]\n",
      "[0.5007649277207766]\n",
      "Derivationnal\n",
      "Normal\n",
      "[0.15630814]\n",
      "[0.6790398803003883]\n",
      "Normal NNP\n",
      "[0.0874205]\n",
      "[0.4984289860111325]\n",
      "CC intra\n",
      "[0.12175258]\n",
      "[0.5003503237238648]\n",
      "CC inter\n",
      "[0.15171589]\n",
      "[0.5005767901574866]\n",
      "Random->Normal\n",
      "[0.07464414473623036]\n",
      "[0.500759367018961]\n",
      "Normal->Random\n",
      "[0.05997217817232013]\n",
      "[0.50049051736413]\n",
      "Inflectional\n",
      "Normal\n",
      "[0.2951204]\n",
      "[0.8506725440174288]\n",
      "Normal NNP\n",
      "[0.111265816]\n",
      "[0.49970194231963355]\n",
      "CC intra\n",
      "[0.15105441]\n",
      "[0.5041745169262756]\n",
      "CC inter\n",
      "[0.18228066]\n",
      "[0.4992979616192089]\n",
      "Random->Normal\n",
      "[0.08969925144687295]\n",
      "[0.5003718090045324]\n",
      "Normal->Random\n",
      "[0.06294568644836546]\n",
      "[0.4996029461753017]\n",
      "Random->Random\n",
      "[0.00047584550629835575]\n",
      "[0.5004407848280471]\n"
     ]
    }
   ],
   "source": [
    "#word2vec roc\n",
    "\n",
    "labels=[\"Lexicographic\",\"Encyclopedic\",\"Derivationnal\",\"Inflectional\"]\n",
    "intervals=[(0,10),(10,20),(20,30),(30,40)]\n",
    "x = (ocs, ocs_nnp, ocs_categ_categ_intra, ocs_categ_categ, ocs_random_normal, ocs_normal_random, ocs_random_random)\n",
    "y = (pcs, pcs_nnp, pcs_categ_categ_intra, pcs_categ_categ, pcs_random_normal, pcs_normal_random, pcs_random_random)\n",
    "\n",
    "\n",
    "x_n, x_nnp, x_categ_categ_intra, x_categ_categ, x_random_normal, x_normal_random, x_random_random = x\n",
    "x_random_normal, x_normal_random, x_random_random = np.mean(x_random_normal,axis=0), np.mean(x_normal_random,axis=0), np.mean(x_random_random,axis=0)\n",
    "y_n, y_nnp, y_categ_categ_intra, y_categ_categ, y_random_normal, y_normal_random, y_random_random = y\n",
    "y_random_normal, y_normal_random, y_random_random = np.mean(y_random_normal,axis=0), np.mean(y_normal_random,axis=0), np.mean(y_random_random,axis=0)\n",
    "\n",
    "def l_m(x):\n",
    "    return([np.mean(x)])\n",
    "    \n",
    "for i in range(len(labels)):\n",
    "    i1,i2 = intervals[i]\n",
    "    x_cc = x_categ_categ[i1:i2]\n",
    "    y_cc = x_categ_categ[i1:i2]\n",
    "    for icc in perm_list_categ_categ:\n",
    "        if perm_list_categ_categ[icc] in range(i1,i2):\n",
    "            x_cc = np.hstack((x_cc, x_categ_categ[icc]))\n",
    "            y_cc = np.hstack((y_cc, y_categ_categ[icc]))\n",
    "    print(labels[i])\n",
    "    print(\"Normal\")\n",
    "    print(l_m(x_n[i1:i2]))\n",
    "    print(l_m(y_n[i1:i2]))\n",
    "    print(\"Normal NNP\")\n",
    "    print(l_m(x_nnp[i1:i2]))\n",
    "    print(l_m(y_nnp[i1:i2]))\n",
    "    print(\"CC intra\")\n",
    "    print(l_m(x_categ_categ_intra[i1:i2]))\n",
    "    print(l_m(y_categ_categ_intra[i1:i2]))\n",
    "    print(\"CC inter\")\n",
    "    print(l_m(x_categ_categ[i1:i2]))\n",
    "    print(l_m(y_categ_categ[i1:i2]))\n",
    "    print(\"Random->Normal\")\n",
    "    print(l_m(x_normal_random[i1:i2]))\n",
    "    print(l_m(y_normal_random[i1:i2]))\n",
    "    print(\"Normal->Random\")\n",
    "    print(l_m(x_random_normal[i1:i2]))\n",
    "    print(l_m(y_random_normal[i1:i2]))\n",
    "print(\"Random->Random\")\n",
    "print(l_m(x_random_random))\n",
    "print(l_m(y_random_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicographic\n",
      "Normal 0.5379290215368527\n",
      "Normal NNP 0.5000666601083112\n",
      "CC intra 0.500158897322328\n",
      "CC inter 0.48943239868336674\n",
      "Random->Normal 0.5000518325593741\n",
      "Normal->Random 0.4993168933866512\n",
      "Encyclopedic\n",
      "Normal 0.5566167896026564\n",
      "Normal NNP 0.5001332847163477\n",
      "CC intra 0.501470630889484\n",
      "CC inter 0.3226533744580055\n",
      "Random->Normal 0.4993094037794368\n",
      "Normal->Random 0.5006965934595644\n",
      "Derivationnal\n",
      "Normal 0.6790253152647141\n",
      "Normal NNP 0.4997700469288904\n",
      "CC intra 0.4996664562434659\n",
      "CC inter nan\n",
      "Random->Normal 0.5004644784300808\n",
      "Normal->Random 0.5003988574826186\n",
      "Inflectional\n",
      "Normal 0.8509698953960825\n",
      "Normal NNP 0.4991496151082746\n",
      "CC intra 0.500518839172449\n",
      "CC inter nan\n",
      "Random->Normal 0.5003427146784183\n",
      "Normal->Random 0.49954779690872\n",
      "Random->Random 0.50066296079548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice.\n",
      "\n",
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(labels)):\n",
    "    i1,i2 = intervals[i]\n",
    "    y_cc = y_categ_categ[i1:i2]\n",
    "    for icc in perm_list_categ_categ:\n",
    "        if perm_list_categ_categ[icc] in range(i1,i2):\n",
    "            y_cc = np.hstack((y_cc, y_categ_categ[icc]))\n",
    "    print(labels[i])\n",
    "    print(\"Normal\", l_m(y_n[i1:i2])[0])\n",
    "    print(\"Normal NNP\", l_m(y_nnp[i1:i2])[0])\n",
    "    print(\"CC intra\", l_m(y_categ_categ_intra[i1:i2])[0])\n",
    "    print(\"CC inter\", l_m(y_cc[i1:i2])[0])\n",
    "    print(\"Random->Normal\", l_m(y_normal_random[i1:i2])[0])\n",
    "    print(\"Normal->Random\", l_m(y_random_normal[i1:i2])[0])\n",
    "print(\"Random->Random\", l_m(y_random_random)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicographic\n",
      "Normal 0.5374205051995696\n",
      "Normal NNP 0.5000064274469275\n",
      "CC intra 0.500972445498127\n",
      "CC inter 0.5020916179998232\n",
      "Random->Normal 0.5000440911490568\n",
      "Normal->Random 0.4994701837967577\n",
      "Encyclopedic\n",
      "Normal 0.5565923947940875\n",
      "Normal NNP 0.49930119761268843\n",
      "CC intra 0.49648172922058953\n",
      "CC inter 0.49914245428597975\n",
      "Random->Normal 0.4996925414543981\n",
      "Normal->Random 0.5007649277207766\n",
      "Derivationnal\n",
      "Normal 0.6790398803003883\n",
      "Normal NNP 0.4984289860111325\n",
      "CC intra 0.5003503237238648\n",
      "CC inter 0.5005767901574866\n",
      "Random->Normal 0.500759367018961\n",
      "Normal->Random 0.50049051736413\n",
      "Inflectional\n",
      "Normal 0.8506725440174288\n",
      "Normal NNP 0.49970194231963355\n",
      "CC intra 0.5041745169262756\n",
      "CC inter 0.4992979616192089\n",
      "Random->Normal 0.5003718090045324\n",
      "Normal->Random 0.4996029461753017\n",
      "Random->Random 0.5004407848280471\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(labels)):\n",
    "    i1,i2 = intervals[i]\n",
    "    y_cc = x_categ_categ[i1:i2]\n",
    "    for icc in perm_list_categ_categ:\n",
    "        if perm_list_categ_categ[icc] in range(i1,i2):\n",
    "            y_cc = np.hstack((y_cc, y_categ_categ[icc]))\n",
    "    print(labels[i])\n",
    "    print(\"Normal\", l_m(y_n[i1:i2])[0])\n",
    "    print(\"Normal NNP\", l_m(y_nnp[i1:i2])[0])\n",
    "    print(\"CC intra\", l_m(y_categ_categ_intra[i1:i2])[0])\n",
    "    print(\"CC inter\", l_m(y_categ_categ[i1:i2])[0])\n",
    "    print(\"Random->Normal\", l_m(y_normal_random[i1:i2])[0])\n",
    "    print(\"Normal->Random\", l_m(y_random_normal[i1:i2])[0])\n",
    "print(\"Random->Random\", l_m(y_random_random)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_ofmodel(model, directions_names, directions_tuples, nb_perms=50, token=False, tokenizer=None):\n",
    "    \n",
    "\n",
    "    if token:\n",
    "        directions = [list(d) for d in directions_tuples]\n",
    "        #directions = [[d for d in list(directions_tuples[i]) if d[0] in vocabulary and d[1] in vocabulary] for i in range(len(directions_tuples))]\n",
    "        direction_w2vs = np.array([[token_embedding(tokenizer, model, i[1]) - token_embedding(tokenizer, model, i[0]) \n",
    "                                    for i in directions[k]] \n",
    "                                    for k in range(len(directions))])\n",
    "        #direction_w2vs_shuffle = np.array([[token_embedding(tokenizer, model, i[1]) - token_embedding(tokenizer, model, j[0])\n",
    "        #                                    for i in directions_tuples[k]\n",
    "        #                                    for j in directions_tuples[k] if i!=j  and i[1]!=j[1]] \n",
    "        #                           for k in range(len(directions_tuples))])\n",
    "        direction_w2vs_shuffle = []\n",
    "        for k in range(len(directions)):\n",
    "            direction_w2vs_shuffle.append([])\n",
    "            for perm in range(nb_perms):\n",
    "                #perm_list = permutation_onecycle(len(directions[k]))\n",
    "                perm_list = permutation_onecycle_avoidtrue(len(directions[k]),  directions[k])\n",
    "                dirs = [token_embedding(tokenizer, model, directions[k][perm_list[i]][1]) - token_embedding(tokenizer, model, directions[k][i][0])\n",
    "                                                      for i in range(len(directions[k]))]\n",
    "                direction_w2vs_shuffle[-1].append(dirs)\n",
    "    else:\n",
    "        vocabulary_keys = model.wv.vocab.keys()\n",
    "        vocabulary = set(vocabulary_keys)\n",
    "\n",
    "        #list_directions_tuples = [list(d) for d in directions_tuples]\n",
    "        directions = [[d for d in list(directions_tuples[i]) if d[0] in vocabulary and d[1] in vocabulary] for i in range(len(directions_tuples))]\n",
    "        \n",
    "        \n",
    "        direction_w2vs = np.array([[model.wv.get_vector(i[1]) - model.wv.get_vector(i[0]) \n",
    "                                    for i in directions[k] if i[1] in vocabulary and i[0] in vocabulary] \n",
    "                                   for k in range(len(directions))])\n",
    "        #direction_w2vs_shuffle = np.array([[model.wv.get_vector(i[1]) - model.wv.get_vector(j[0]) \n",
    "        #                                for i in directions_tuples[k] if i[1] in vocabulary \n",
    "        #                                for j in directions_tuples[k] if j[0] in vocabulary and i!=j]  \n",
    "        #                       for k in range(len(directions_tuples))])\n",
    "        direction_w2vs_shuffle = []\n",
    "        for k in range(len(directions)):\n",
    "            direction_w2vs_shuffle.append([])\n",
    "            for perm in range(nb_perms):\n",
    "                #perm_list = permutation_onecycle(len(directions_tuples[k]))\n",
    "                perm_list = permutation_onecycle_avoidtrue(len(directions[k]),  directions[k])\n",
    "                dirs = [model.wv.get_vector(directions[k][perm_list[i]][1]) - model.wv.get_vector(directions[k][i][0])\n",
    "                                                      for i in range(len(directions[k]))]\n",
    "                direction_w2vs_shuffle[-1].append(dirs)\n",
    "    \n",
    "    directions_similarity_w2vs = similarite_offsets(directions, direction_w2vs)\n",
    "    directions_similarity_w2vs_shuffle = [similarite_offsets(directions, np.array(direction_w2vs_shuffle)[:,perm]) for perm in range(nb_perms)]\n",
    "    \n",
    "    ocs, pcs = OCS_PCS(len(directions_names), \n",
    "                       nb_perms,\n",
    "                       directions_similarity_w2vs, \n",
    "                       directions_similarity_w2vs_shuffle)\n",
    "    \n",
    "    return(ocs, pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('introvert', 'extravert')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directions = [list(d) for d in directions_tuples]\n",
    "directions[3][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n",
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n",
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n",
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n",
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict2vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n",
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n",
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numberbatch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n",
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n",
      "/home/lfournier/.conda/envs/py_lf/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "free variable 'vocabulary' referenced before assignment in enclosing scope",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-306-40b95c585d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mscores_models2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_ofmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_conceptnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirections_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirections_tuples_bats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mscores_models2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_ofmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirections_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirections_tuples_bats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mscores_models2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_ofmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt2_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirections_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirections_tuples_bats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpt2_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-301-4880bf56c543>\u001b[0m in \u001b[0;36mmetrics_ofmodel\u001b[0;34m(model, directions_names, directions_tuples, nb_perms, token, tokenizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#directions = [list(d) for d in directions_tuples]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdirections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections_tuples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         direction_w2vs = np.array([[token_embedding(tokenizer, model, i[1]) - token_embedding(tokenizer, model, i[0]) \n\u001b[1;32m      8\u001b[0m                                     for i in directions[k]] \n",
      "\u001b[0;32m<ipython-input-301-4880bf56c543>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#directions = [list(d) for d in directions_tuples]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdirections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections_tuples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         direction_w2vs = np.array([[token_embedding(tokenizer, model, i[1]) - token_embedding(tokenizer, model, i[0]) \n\u001b[1;32m      8\u001b[0m                                     for i in directions[k]] \n",
      "\u001b[0;32m<ipython-input-301-4880bf56c543>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#directions = [list(d) for d in directions_tuples]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdirections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections_tuples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         direction_w2vs = np.array([[token_embedding(tokenizer, model, i[1]) - token_embedding(tokenizer, model, i[0]) \n\u001b[1;32m      8\u001b[0m                                     for i in directions[k]] \n",
      "\u001b[0;31mNameError\u001b[0m: free variable 'vocabulary' referenced before assignment in enclosing scope"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "scores_models2 = []\n",
    "print(\"w2v\")\n",
    "scores_models2.append((metrics_ofmodel(model, directions_names, directions_tuples_bats)))\n",
    "print(\"glove\")\n",
    "scores_models2.append((metrics_ofmodel(model_glove, directions_names, directions_tuples_bats)))\n",
    "print(\"dict2vec\")\n",
    "scores_models2.append((metrics_ofmodel(model_dict2vec, directions_names, directions_tuples_bats)))\n",
    "print(\"numberbatch\")\n",
    "scores_models2.append((metrics_ofmodel(model_conceptnet, directions_names, directions_tuples_bats)))\n",
    "print(\"bert\")\n",
    "scores_models2.append((metrics_ofmodel(bert_model, directions_names, directions_tuples_bats, token=True, tokenizer=bert_tokenizer)))\n",
    "print(\"gpt2\")\n",
    "scores_models2.append((metrics_ofmodel(gpt2_model, directions_names, directions_tuples_bats, token=True, tokenizer=gpt2_tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_models = [i[1] for i in scores_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec\n",
      "Derivationnal 0.15630814 0.6790723348103488\n",
      "Encyclopedic 0.19812895 0.5591797316771174\n",
      "Inflectional 0.2951204 0.850800472832011\n",
      "Lexicographic 0.031050116 0.539257017823826\n",
      "Glove\n",
      "Derivationnal 0.23739421 0.7100789384423158\n",
      "Encyclopedic 0.2552645 0.6231332902173314\n",
      "Inflectional 0.3447476 0.8595064435991357\n",
      "Lexicographic 0.043298054 0.5496225486047479\n",
      "dict2vec\n",
      "Derivationnal 0.07907895 0.6291216193241242\n",
      "Encyclopedic 0.21264341 0.6279168959600167\n",
      "Inflectional 0.09856267 0.7001493223209936\n",
      "Lexicographic 0.024130624 0.5340288299875052\n",
      "Numberbatch\n",
      "Derivationnal 0.2242213 0.8052135369981617\n",
      "Encyclopedic 0.25081062 0.673627129555972\n",
      "Inflectional 0.35681745 0.9239420923399727\n",
      "Lexicographic 0.03379402 0.5520509241149522\n",
      "BERT\n",
      "Derivationnal 0.17828079 0.6318017206164098\n",
      "Encyclopedic 0.1511009 0.5687281932528113\n",
      "Inflectional 0.21723476 0.8206673927611924\n",
      "Lexicographic 0.016187701 0.5166289879216993\n",
      "GPT-2\n",
      "Derivationnal 0.27039167 0.7320427188671387\n",
      "Encyclopedic 0.07054619 0.5131203005414411\n",
      "Inflectional 0.09746338 0.6505108177701392\n",
      "Lexicographic 0.0108071845 0.5080856316534776\n"
     ]
    }
   ],
   "source": [
    "ocs_models = [i[0] for i in scores_models2]\n",
    "pcs_models = [i[1] for i in scores_models2]\n",
    "\n",
    "#intervals = [[0,10],[10,20],[20,30],[30,40]]\n",
    "intervals=[(20,30),(10,20),(30,40),(0,10)]\n",
    "for i_m in range(len(models)):\n",
    "    print(models[i_m])\n",
    "    for i_l in range(len(labels)):\n",
    "        #print(labels[i_l])\n",
    "        i,j = intervals[i_l]\n",
    "        print(labels[i_l], l_m(ocs_models[i_m][i:j])[0], l_m(pcs_models[i_m][i:j])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"7f342e47-cb2d-49f9-8daf-ebd85079ffea\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"7f342e47-cb2d-49f9-8daf-ebd85079ffea\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '7f342e47-cb2d-49f9-8daf-ebd85079ffea',\n",
       "                        [{\"marker\": {\"color\": \"#FF7070\"}, \"name\": \"word2vec\", \"type\": \"bar\", \"x\": [\"Lexicographic\", \"Encyclopedic\", \"Derivationnal\", \"Inflectional\"], \"y\": [0.5374205051995696, 0.5565923947940875, 0.6790398803003883, 0.8506725440174288]}, {\"marker\": {\"color\": \"#FF4136\"}, \"name\": \"Glove\", \"type\": \"bar\", \"x\": [\"Lexicographic\", \"Encyclopedic\", \"Derivationnal\", \"Inflectional\"], \"y\": [0.5493359194008598, 0.6224346085564857, 0.7099931644790617, 0.8585975854755145]}, {\"marker\": {\"color\": \"#CAE000\"}, \"name\": \"dict2vec\", \"type\": \"bar\", \"x\": [\"Lexicographic\", \"Encyclopedic\", \"Derivationnal\", \"Inflectional\"], \"y\": [0.5329543306001188, 0.6215538915684686, 0.6298579254408814, 0.7003556062058321]}, {\"marker\": {\"color\": \"#00610F\"}, \"name\": \"Numberbatch\", \"type\": \"bar\", \"x\": [\"Lexicographic\", \"Encyclopedic\", \"Derivationnal\", \"Inflectional\"], \"y\": [0.5511810192446908, 0.6638511754693638, 0.8048451022960037, 0.9241313723717601]}, {\"marker\": {\"color\": \"#39CCCC\"}, \"name\": \"BERT\", \"type\": \"bar\", \"x\": [\"Lexicographic\", \"Encyclopedic\", \"Derivationnal\", \"Inflectional\"], \"y\": [0.5164310550603914, 0.5649062783840066, 0.6318140354852145, 0.8208116815656142]}, {\"marker\": {\"color\": \"#5F2EFF\"}, \"name\": \"GPT-2\", \"type\": \"bar\", \"x\": [\"Lexicographic\", \"Encyclopedic\", \"Derivationnal\", \"Inflectional\"], \"y\": [0.5077972298209079, 0.5126891408579758, 0.7319629241149521, 0.6503520185087988]}],\n",
       "                        {\"barmode\": \"group\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"x\": 0.5, \"xanchor\": \"center\", \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Category type\"}}, \"yaxis\": {\"title\": {\"text\": \"Pairwise Consistency Score\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('7f342e47-cb2d-49f9-8daf-ebd85079ffea');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "models=['word2vec', 'Glove', 'dict2vec', 'Numberbatch', 'BERT', 'GPT-2']\n",
    "labels=[\"Lexicographic\",\"Encyclopedic\",\"Derivationnal\",\"Inflectional\"]\n",
    "c = ['#FF7070','#FF4136','#CAE000','#00610F', '#39CCCC', '#5F2EFF']\n",
    "intevals = [[0,10],[10,20],[20,30],[30,40]]\n",
    "\n",
    "data = [go.Bar(name=models[i], x=labels, y=[l_m(pcs_models[i][0:10])[0], \n",
    "                                            l_m(pcs_models[i][10:20])[0], \n",
    "                                            l_m(pcs_models[i][20:30])[0], \n",
    "                                            l_m(pcs_models[i][30:40])[0]],\n",
    "              marker_color=c[i]\n",
    "              ) for i in range(len(models))]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title={\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title_text=\"Category type\", # xaxis label\n",
    "    yaxis_title_text='Pairwise Consistency Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEL\n",
    "y_analogie = np.array([[.686,.645,.503,.360,.568,.119],\n",
    "    [.158,.167,.127,.060,.288,.359],\n",
    "    [.198, .281, .162, .056, .041, .014],\n",
    "    [.063, .080, .030, .004, .002, .003]]).T\n",
    "\n",
    "y_pcs = np.array([[.851, .860, .700, .924, .821, .651],\n",
    "    [.679, .710, .630, .805, .632, .732],\n",
    "    [.559, .623, .628, .674, .569, .513],\n",
    "    [.539, .550, .534, .552, .517, .508]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"afdf7a2f-964f-4f69-8929-3bd1e663b8c8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"afdf7a2f-964f-4f69-8929-3bd1e663b8c8\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'afdf7a2f-964f-4f69-8929-3bd1e663b8c8',\n",
       "                        [{\"marker\": {\"color\": \"#FF7070\"}, \"name\": \"word2vec\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.851, 0.679, 0.559, 0.539]}, {\"marker\": {\"color\": \"#FF4136\"}, \"name\": \"Glove\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.86, 0.71, 0.623, 0.55]}, {\"marker\": {\"color\": \"#CAE000\"}, \"name\": \"dict2vec\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.7, 0.63, 0.628, 0.534]}, {\"marker\": {\"color\": \"#00610F\"}, \"name\": \"Numberbatch\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.924, 0.805, 0.674, 0.552]}, {\"marker\": {\"color\": \"#39CCCC\"}, \"name\": \"BERT\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.821, 0.632, 0.569, 0.517]}, {\"marker\": {\"color\": \"#5F2EFF\"}, \"name\": \"GPT-2\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.651, 0.732, 0.513, 0.508]}],\n",
       "                        {\"barmode\": \"group\", \"font\": {\"family\": \"Times New Roman\", \"size\": 26}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"x\": 0.5, \"xanchor\": \"center\", \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Category type\"}}, \"yaxis\": {\"range\": [0.5, 1], \"title\": {\"text\": \"Pairwise Consistency Score\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('afdf7a2f-964f-4f69-8929-3bd1e663b8c8');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "models=['word2vec', 'Glove', 'dict2vec', 'Numberbatch', 'BERT', 'GPT-2']\n",
    "labels=[\"Inflectional\", \"Derivational\",\"Encyclopedic\", \"Lexicographic\"]\n",
    "c = ['#FF7070','#FF4136','#CAE000','#00610F', '#39CCCC', '#5F2EFF']\n",
    "intevals = [[30,40],[20,30],[10,20], [0,10]]\n",
    "\n",
    "#data = [go.Bar(name=models[i], x=labels, y=[l_m(pcs_models[i][0:10])[0], \n",
    "#                                            l_m(pcs_models[i][10:20])[0], \n",
    "#                                            l_m(pcs_models[i][20:30])[0], \n",
    "#                                            l_m(pcs_models[i][30:40])[0]],\n",
    "data = [go.Bar(name=models[i], x=labels, y=y_pcs[i],\n",
    "marker_color=c[i]\n",
    "              ) for i in range(len(models))]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "#fig.add_trace(go.Scatter(x=labels, y=[0.5,0.5,0.5,0.5],\n",
    "#                         line = dict(color='royalblue', width=4, dash='dash')))\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title={\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    font=dict(family=\"Times New Roman\",size=26),\n",
    "    #uniformtext_minsize=24,\n",
    "        #family=\"Courier New, monospace\",\n",
    "        #size=18,\n",
    "        #color=\"RebeccaPurple\"\n",
    "    #)\n",
    "    xaxis_title_text=\"Category type\", # xaxis label\n",
    "    yaxis_title_text='Pairwise Consistency Score'\n",
    ")\n",
    "fig.update_yaxes(range=[0.5, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"59d57960-c998-4491-aeda-92df88bd8a22\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"59d57960-c998-4491-aeda-92df88bd8a22\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '59d57960-c998-4491-aeda-92df88bd8a22',\n",
       "                        [{\"marker\": {\"color\": \"#FF7070\"}, \"name\": \"word2vec\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.686, 0.158, 0.198, 0.063]}, {\"marker\": {\"color\": \"#FF4136\"}, \"name\": \"Glove\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.645, 0.167, 0.281, 0.08]}, {\"marker\": {\"color\": \"#CAE000\"}, \"name\": \"dict2vec\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.503, 0.127, 0.162, 0.03]}, {\"marker\": {\"color\": \"#00610F\"}, \"name\": \"Numberbatch\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.36, 0.06, 0.056, 0.004]}, {\"marker\": {\"color\": \"#39CCCC\"}, \"name\": \"BERT\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.568, 0.288, 0.041, 0.002]}, {\"marker\": {\"color\": \"#5F2EFF\"}, \"name\": \"GPT-2\", \"type\": \"bar\", \"x\": [\"Inflectional\", \"Derivational\", \"Encyclopedic\", \"Lexicographic\"], \"y\": [0.119, 0.359, 0.014, 0.003]}],\n",
       "                        {\"barmode\": \"group\", \"font\": {\"family\": \"Times New Roman\", \"size\": 26}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"x\": 0.5, \"xanchor\": \"center\", \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Category type\"}}, \"yaxis\": {\"range\": [0, 1], \"title\": {\"text\": \"Analogy test accuracy\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('59d57960-c998-4491-aeda-92df88bd8a22');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "models=['word2vec', 'Glove', 'dict2vec', 'Numberbatch', 'BERT', 'GPT-2']\n",
    "labels=[\"Inflectional\", \"Derivational\",\"Encyclopedic\", \"Lexicographic\"]\n",
    "c = ['#FF7070','#FF4136','#CAE000','#00610F', '#39CCCC', '#5F2EFF']\n",
    "intevals = [[30,40],[20,30],[10,20], [0,10]]\n",
    "\n",
    "#data = [go.Bar(name=models[i], x=labels, y=[l_m(pcs_models[i][0:10])[0], \n",
    "#                                            l_m(pcs_models[i][10:20])[0], \n",
    "#                                            l_m(pcs_models[i][20:30])[0], \n",
    "#                                            l_m(pcs_models[i][30:40])[0]],\n",
    "data = [go.Bar(name=\"models[i]\", x=labels, y=y_analogie[i],\n",
    "marker_color=c[i]\n",
    "              ) for i in range(len(models))]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "#fig.add_trace(go.Scatter(x=labels, y=[0.5,0.5,0.5,0.5],\n",
    "#                         line = dict(color='royalblue', width=4, dash='dash')))\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title={\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    font=dict(family=\"Times New Roman\",size=26),\n",
    "    #uniformtext_minsize=24,\n",
    "        #family=\"Courier New, monospace\",\n",
    "        #size=18,\n",
    "        #color=\"RebeccaPurple\"\n",
    "    #)\n",
    "    xaxis_title_text=\"Category type\", # xaxis label\n",
    "    yaxis_title_text='Analogy test accuracy'\n",
    ")\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"6ff4993e-59c8-43e6-a932-bdbc5c60f568\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"6ff4993e-59c8-43e6-a932-bdbc5c60f568\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '6ff4993e-59c8-43e6-a932-bdbc5c60f568',\n",
       "                        [{\"marker\": {\"color\": \"#FF7070\"}, \"name\": \"Lexicographic\", \"type\": \"bar\", \"x\": [\"word2vec\", \"Glove\", \"dict2vec\", \"Numberbatch\", \"BERT\", \"GPT-2\"], \"y\": [0.5374205051995696, 0.5493359194008598, 0.5329543306001188, 0.5511810192446908, 0.5164310550603914, 0.5077972298209079]}, {\"marker\": {\"color\": \"#00610F\"}, \"name\": \"Encyclopedic\", \"type\": \"bar\", \"x\": [\"word2vec\", \"Glove\", \"dict2vec\", \"Numberbatch\", \"BERT\", \"GPT-2\"], \"y\": [0.5565923947940875, 0.6224346085564857, 0.6215538915684686, 0.6638511754693638, 0.5649062783840066, 0.5126891408579758]}, {\"marker\": {\"color\": \"#CAE000\"}, \"name\": \"Derivationnal\", \"type\": \"bar\", \"x\": [\"word2vec\", \"Glove\", \"dict2vec\", \"Numberbatch\", \"BERT\", \"GPT-2\"], \"y\": [0.6790398803003883, 0.7099931644790617, 0.6298579254408814, 0.8048451022960037, 0.6318140354852145, 0.7319629241149521]}, {\"marker\": {\"color\": \"#5F2EFF\"}, \"name\": \"Inflectional\", \"type\": \"bar\", \"x\": [\"word2vec\", \"Glove\", \"dict2vec\", \"Numberbatch\", \"BERT\", \"GPT-2\"], \"y\": [0.8506725440174288, 0.8585975854755145, 0.7003556062058321, 0.9241313723717601, 0.8208116815656142, 0.6503520185087988]}],\n",
       "                        {\"barmode\": \"group\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"x\": 0.5, \"xanchor\": \"center\", \"y\": 0.9, \"yanchor\": \"top\"}, \"xaxis\": {\"title\": {\"text\": \"Word embeddings\"}}, \"yaxis\": {\"title\": {\"text\": \"Pairwise Consistency Score\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('6ff4993e-59c8-43e6-a932-bdbc5c60f568');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = ['#FF7070','#00610F','#CAE000','#5F2EFF']\n",
    "\n",
    "data = [go.Bar(name=labels[i], x=models, y=[l_m(pcs_models[k][intervals[i][0]:intervals[i][1]])[0] for k in range(len(models))],\n",
    "              marker_color=c[i]\n",
    "              ) for i in range(len(labels))]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title={\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title_text=\"Word embeddings\", # xaxis label\n",
    "    yaxis_title_text='Pairwise Consistency Score')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
